{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Talos Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf1\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import tf_robot_learning as rl\n",
    "import tf_robot_learning.distributions as ds\n",
    "import os, time\n",
    "from tf_robot_learning import kinematic as tk\n",
    "\n",
    "from IPython.core import display\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Define and Train GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf1.InteractiveSession()\n",
    "tf1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define robot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urdf = tk.urdf_from_file(rl.datapath + '/urdf/talos_reduced.urdf');\n",
    "display.clear_output()\n",
    "\n",
    "#list of end-effector\n",
    "tips = OrderedDict({\n",
    "    'r_gripper'\t: 'gripper_right_base_link',\n",
    "    'l_gripper'\t: 'gripper_left_base_link',\n",
    "    'r_foot' \t: 'right_sole_link',\n",
    "    'l_foot' \t: 'left_sole_link',\n",
    "})\n",
    "\n",
    "#define the robot as a kinematic chain, loaded from urdf\n",
    "chain_names = ['r_gripper', 'r_foot', 'l_foot'] \n",
    "chain = tk.ChainDict({\n",
    "    name: tk.kdl_chain_from_urdf_model(urdf, 'base_link', tip=tip)\n",
    "    for name, tip in tips.items()\n",
    "})\n",
    "\n",
    "#define the default position and orientation of the end-effector\n",
    "ee = OrderedDict({\n",
    "        'l_gripper': [0.  , 0.29, 0.8 , 1.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  , 0.  ,1.  ],\n",
    "        'r_gripper': [0.  , -0.29, 0.8 , 1.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  , 0.  ,1.  ],\n",
    "        'l_foot': [-0.02,  0.09, -0.  ,  1.  ,  0.  ,  0.  ,  0.  ,  1.  ,  0.  , 0.  ,  0.  ,  1.  ],\n",
    "        'r_foot': [-0.02,  -0.09, -0.  ,  1.  ,  0.  ,  0.  ,  0.  ,  1.  ,  0.  , 0.  ,  0.  ,  1.  ],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_to_joint_pos(x):\n",
    "    \"\"\"\n",
    "    convert the config parameter x to full robot configurations: joint angles, base position and orientation\n",
    "    \"\"\"\n",
    "    return x[..., :chain.nb_joint],\\\n",
    "        tf.concat([x[..., chain.nb_joint:chain.nb_joint+3]], -1),\\\n",
    "        tk.rotation.rpy(tf.zeros_like(x[..., -3:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def q_augmented(q):\n",
    "    \"\"\" Define augmented data transformations: (joint_angles, right foot pose, and left foot pose)\"\"\"\n",
    "    _q, _p, _m = param_to_joint_pos(q)\n",
    "    return tf.concat([\n",
    "            q,\n",
    "            chain.xs(_q, floating_base=(_p, _m), name='r_foot')[:, -1],\n",
    "            chain.xs(_q, floating_base=(_p, _m), name='l_foot')[:, -1]\n",
    "        ], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def q_target(q):\n",
    "    \"\"\" Extract the target\"\"\"\n",
    "    _q, _p, _m = param_to_joint_pos(q)\n",
    "    return tf.concat( # orientation of each foot\n",
    "            [chain.xs(_q, floating_base=(_p, _m), name=name)[:, -1, :3] for name in chain_names]\n",
    "            , -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def q_foot_ori(q):\n",
    "    \"\"\" Extract the foot orientation \"\"\"\n",
    "    _q, _p, _m = param_to_joint_pos(q)\n",
    "    return tf.concat( # position of each end-effector\n",
    "            [chain.xs(_q, floating_base=(_p, _m), name=name)[:, -1, 3:] for name in chain_names[1:]]\n",
    "            , -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mini batch\n",
    "def get_batch(_batch_size=30, cut=None, augmented=True):\n",
    "    if cut is not None: idx = np.random.randint(0, cut, _batch_size)\n",
    "    else: idx = np.random.randint(0, data_augmented.shape[0]-1, _batch_size)\n",
    "    if augmented: return data_augmented[idx]\n",
    "    else: return data[idx]\n",
    "    \n",
    "def get_target_batch(_batch_size=30, cut=None):\n",
    "    if cut is not None: idx = np.random.randint( 0, cut, _batch_size)\n",
    "    else: idx = np.random.randint(0, data_augmented.shape[0]-1, _batch_size)\n",
    "    return data_target[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "q_eval = tf1.placeholder(tf.float32, (None, chain.nb_joint + 3))\n",
    "\n",
    "q_augmented_eval = q_augmented(q_eval)\n",
    "q_target_eval = q_target(q_eval)\n",
    "q_foot_ori_eval = q_foot_ori(q_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dataset\n",
    "data = np.load('data/data_two_feet_manual.npy')\n",
    "# compute data through these transformations\n",
    "data_augmented = q_augmented_eval.eval({q_eval: data})\n",
    "data_target = q_target_eval.eval({q_eval: data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "joint_dim = chain.nb_joint + 3 # the configuration consists of nb_joint= 28 joint angles and 3 base position\n",
    "latent_dim = 30 # dimension of noise\n",
    "N_net = 5\n",
    "target_dim = 9  # size of the target\n",
    "augmented_dim = 2 * 12 + joint_dim # size of augmented data (joint angles + poses of both foot)\n",
    "\n",
    "n_input = latent_dim + target_dim #dimension of the input to the generator\n",
    "batch_size = tf1.placeholder(tf.int32, ())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_nn = rl.nn.MLP(\n",
    "    n_input=n_input, n_output=joint_dim, n_hidden=[200, 200],\n",
    "    act_fct=tf.nn.relu, batch_size_svi=N_net\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the generator input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise to feed generator\n",
    "eps_var = tf1.placeholder(tf.float32, ())\n",
    "eps = tf.random.normal([tf.cast(batch_size/N_net, tf.int32), latent_dim], \n",
    "    dtype=tf.float32, mean=0., stddev=eps_var, name='epsilon')\n",
    "\n",
    "#the input to generator = noise + target\n",
    "batch_target = tf1.placeholder(tf.float32, (None, target_dim))\n",
    "eps_conc = tf.concat([eps, batch_target], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define output transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_q = tf.reshape(gen_nn.pred(eps_conc) + tf.constant(chain.mean_pose + [0,0,1.08])[None], (-1, joint_dim))\n",
    "\n",
    "samples_qq, samples_p, samples_m = param_to_joint_pos(samples_q)\n",
    "samples_links, _, samples_com = chain.xs(samples_qq, floating_base=(samples_p, samples_m), get_links=True)\n",
    "\n",
    "samples_augmented = q_augmented(samples_q)\n",
    "samples_target = q_target(samples_q)\n",
    "samples_foot_ori = q_foot_ori(samples_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "discr_nn = rl.nn.MLP(\n",
    "    n_input=augmented_dim, n_output=1, n_hidden=[40, 40],\n",
    "    act_fct=tf.nn.relu\n",
    ")\n",
    "\n",
    "#batch data\n",
    "batch_x = tf1.placeholder(tf.float32, (None, augmented_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main loss functions (discriminator vs generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "d_fake = discr_nn.pred(samples_augmented)[:, 0]\n",
    "d_true = discr_nn.pred(batch_x)[:, 0]\n",
    "\n",
    "#discriminator loss function\n",
    "loss_d = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(d_true), logits=d_true) + \\\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(d_fake), logits=d_fake)\n",
    "loss_d = tf.reduce_sum(loss_d)\n",
    "\n",
    "#generator (basic) loss function\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "    labels=tf.ones_like(d_fake), logits=d_fake)\n",
    "loss = tf.reduce_sum(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_target_std = tf1.placeholder(tf.float32, ()) \n",
    "\n",
    "p_target = ds.MultivariateNormalFullCovariance(\n",
    "    tf.reshape(tf.ones((N_net, 1,1))*batch_target,(-1, target_dim)), p_target_std**2 * tf.eye(target_dim)\n",
    ")\n",
    "\n",
    "loss_target = tf.reduce_sum(-p_target.log_prob(samples_target))\n",
    "lmbda_target = tf1.placeholder(tf.float32, ())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Foot orientation loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "foot_ori_dim = 18\n",
    "batch_foot_ori = tf1.placeholder(tf.float32, (None, foot_ori_dim))\n",
    "\n",
    "# define a Gaussian distribution that should be tracked by the system\n",
    "p_foot_ori_std = tf1.placeholder(tf.float32, ()) \n",
    "\n",
    "p_foot_ori = ds.MultivariateNormalFullCovariance(\n",
    "    tf.reshape(tf.ones((N_net, 1,1))*batch_foot_ori,(-1, foot_ori_dim)), p_foot_ori_std**2 * tf.eye(foot_ori_dim)\n",
    ")\n",
    "\n",
    "loss_foot_ori = tf.reduce_sum(-p_foot_ori.log_prob(samples_foot_ori))\n",
    "lmbda_foot_ori = tf1.placeholder(tf.float32, ())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constraints cost: COM and joint limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#joint limit\n",
    "base_pos_limits = np.array([[-0.1, -0.1, 0.6],[0.1, 0.1, 1.4] ]).T\n",
    "config_limits = np.concatenate([chain.joint_limits, base_pos_limits], axis=0)\n",
    "joint_limits = tf.constant(config_limits, dtype=tf.float32)\n",
    "joint_limits_std = 0.05\n",
    "joint_limits_temp = 1.\n",
    "\n",
    "joint_limits_exp = ds.SoftUniformNormalCdf(\n",
    "    low=joint_limits[:, 0],\n",
    "    high=joint_limits[:, 1],\n",
    "    std=joint_limits_std,\n",
    "    temp=joint_limits_temp,\n",
    "    reduce_axis=-1\n",
    ")\n",
    "\n",
    "joint_limit_constraints = tf.reduce_mean(-joint_limits_exp.log_prob(samples_q[:,:chain.nb_joint+3]))\n",
    "\n",
    "\n",
    "#COM\n",
    "com_limits = 0.1\n",
    "com_limits_std = 0.01\n",
    "\n",
    "com_limits_exp = ds.SoftUniformNormalCdf(\n",
    "    low=-com_limits,\n",
    "    high=com_limits,\n",
    "    std=com_limits_std,\n",
    "    temp=1.,\n",
    "    reduce_axis=-1\n",
    ")\n",
    "\n",
    "com_xy = samples_com[:, :2]\n",
    "center_feet = tf.reduce_mean([\n",
    "        samples_links['r_foot'][:, -1, :2], \n",
    "        samples_links['l_foot'][:, -1, :2]\n",
    "        ], axis=0)\n",
    "\n",
    "cost_constraints = tf.reduce_mean(-com_limits_exp.log_prob(com_xy - center_feet))\n",
    "lmbda_constraints = tf1.placeholder(tf.float32, ())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_gen = loss +  lmbda_target * loss_target + lmbda_foot_ori*loss_foot_ori + lmbda_constraints * (cost_constraints+joint_limit_constraints) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "rate = tf1.placeholder(tf.float32, ())\n",
    "opt = tf1.train.AdamOptimizer\n",
    "\n",
    "optimizer = opt(learning_rate=rate)\n",
    "optimizer_d = opt(learning_rate=rate)\n",
    "\n",
    "train = optimizer.minimize(loss_gen, var_list=gen_nn.vec_weights)\n",
    "train_d = optimizer_d.minimize(loss_d, var_list=discr_nn.vec_weights)\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf1.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_new_batch(_batch_size):\n",
    "    return np.random.multivariate_normal(_targets_m, np.diag(_targets_v ** 2), (_batch_size, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_batch_size = 100\n",
    "_targets_m = np.concatenate([ee[name][:3] for name in chain_names], 0) # mean target\n",
    "_batch_foot_ori = np.tile(np.eye(3).flatten(), (int(_batch_size/N_net),2)).reshape(int(_batch_size/N_net),-1)\n",
    "\n",
    "_targets_m = np.concatenate([ee[name][:3] for name in chain_names], 0) # mean target\n",
    "_targets_v = np.array([0.5] * 3 + [0.01] * 2 + [0.] +  [0.01] * 2 + [0.]  )  # variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "alpha = .7\n",
    "for i in range(2000):\n",
    "    try:\n",
    "        for j in range(5):\n",
    "            # train discriminative_network\n",
    "            _x = get_batch(cut=8000, _batch_size=_batch_size)\n",
    "            \n",
    "            feed_dict = {\n",
    "                lmbda_target: .05,\n",
    "                lmbda_foot_ori: .05,\n",
    "                lmbda_constraints: 5.,\n",
    "                eps_var: 1.,\n",
    "                p_target_std: 0.01,\n",
    "                p_foot_ori_std: 0.01,\n",
    "                batch_x: _x,\n",
    "                batch_size: _batch_size,\n",
    "                rate : 0.002 * alpha,\n",
    "            }\n",
    "            \n",
    "            # from data\n",
    "            feed_dict[batch_target] = get_target_new_batch(_batch_size=int(_batch_size/N_net))\n",
    "\n",
    "            _ = sess.run([train_d], feed_dict=feed_dict)\n",
    "        \n",
    "        feed_dict[rate] = .001 * alpha\n",
    "        feed_dict[batch_foot_ori] = _batch_foot_ori\n",
    "        \n",
    "        # train generative_network\n",
    "        _,  _loss, _loss_d,  _loss_target, _loss_constraints = sess.run(\n",
    "            [train, loss, loss_d,  loss_target, cost_constraints], feed_dict=feed_dict)\n",
    "        \n",
    "        if not i % 10:\n",
    "            display.clear_output(wait=True)\n",
    "            print('Step %i\\t, loss gen %f, loss disc %f,  loss target %f, loss constraints %f' % (i, _loss, _loss_d ,_loss_target, _loss_constraints))\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To save model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "saver = tf1.train.Saver()\n",
    "\n",
    "save_path = saver.save(sess, \"data/talos_foot_fixed_with_data_ensemble2.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf1.train.Saver()\n",
    "saver.restore(sess, \"data/talos_foot_fixed_with_data_ensemble2.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the generated samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "_batch_size = 20\n",
    "feed_dict={batch_size: _batch_size, eps_var:1.}\n",
    "feed_dict[batch_target] = get_target_new_batch(_batch_size=int(_batch_size/N_net))\n",
    "\n",
    "_links, _com , _b_targets = sess.run(\n",
    "[samples_links, samples_com, batch_target], feed_dict\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2)\n",
    "\n",
    "for i in range(2):\n",
    "    dim = [i, 2]\n",
    "    chain.plot(\n",
    "            _links, feed_dict={}, ax=ax[i],\n",
    "            dim=dim, alpha=0.2, color='k'\n",
    "        )    \n",
    "    ax[i].plot(_com[0, dim[0]], _com[0, dim[1]], 'bx')\n",
    "    \n",
    "    for j, name in enumerate(chain_names):\n",
    "        ax[i].plot(_b_targets[:, j*3 + dim[0]], _b_targets[:, j*3 + dim[1]], 'rx', color='orangered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_dataset = 0\n",
    "n = 10\n",
    "\n",
    "_targets_m = np.concatenate([ee[name][:3] for name in chain_names], 0) # mean target\n",
    "_targets_v = np.array([0.6] * 3 + [0.001]*3 + [0.001] * 3  )  # variance\n",
    "\n",
    "if from_dataset:\n",
    "    _targets = get_target_batch(cut=10000, _batch_size=1) * np.ones((int(n/N_net), 1))\n",
    "else:\n",
    "    _targets = np.random.multivariate_normal(_targets_m, np.diag(_targets_v ** 2), (1, )) * np.ones((int(n/N_net), 1))\n",
    "\n",
    "feed_dict={batch_size: n, batch_target: _targets, eps_var:1.}\n",
    "\n",
    "\n",
    "_links, _com , _b_targets, _samples_q = sess.run(\n",
    "    [samples_links, samples_com, batch_target, samples_q], \n",
    "    feed_dict)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(12,8))\n",
    "\n",
    "for i in range(2):\n",
    "    dim = [i, 2]\n",
    "    chain.plot(\n",
    "            _links, feed_dict={}, ax=ax[i],\n",
    "            dim=dim, alpha=0.2, color='k'\n",
    "        )    \n",
    "    \n",
    "    for j, name in enumerate(chain_names):\n",
    "        ax[i].plot(_b_targets[:, j*3 + dim[0]], _b_targets[:, j*3 + dim[1]], 'rx', color='orangered')\n",
    "    ax[i].plot(_com[0, dim[0]], _com[0, dim[1]], 'bx')\n",
    "    #ax[i].set_xlim([-.7, .7])\n",
    "    #x[i].set_ylim([-0.1, 1.2])\n",
    "plt.savefig('data/talos_config.png')\n",
    "plt.show()  \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(ncols=1, figsize=(12,8))\n",
    "dim = [1, 2]\n",
    "chain.plot(\n",
    "        _links, feed_dict={}, ax=ax,\n",
    "        dim=dim, alpha=0.2, color='k'\n",
    "    )    \n",
    "\n",
    "for j, name in enumerate(chain_names):\n",
    "    ax.plot(_b_targets[:, j*3 + dim[0]], _b_targets[:, j*3 + dim[1]], 'rx', color='orangered')\n",
    "ax.plot(_com[0, dim[0]], _com[0, dim[1]], 'bx')\n",
    "ax.set_xlim([-.9, .7])\n",
    "ax.set_ylim([-0.1, 1.6])\n",
    "plt.axis('off')\n",
    "plt.savefig('data/talos_config6.png', bbox_inches='tight', pad_inches=0)\n",
    "plt.show()  \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for sample in _samples_q:\n",
    "    set_q_std(get_pb_config(sample),True)\n",
    "    print(sample[2:9])\n",
    "    input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Use GAN for Projection, IK and Inverse Kinematics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Pybullet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybullet as p\n",
    "import pybullet_data\n",
    "from utils import *\n",
    "from scipy.optimize import fmin_bfgs\n",
    "from costs import *\n",
    "from robot import *\n",
    "from scipy.optimize import fmin_bfgs\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physics_client_id = p.connect(p.GUI)\n",
    "\n",
    "p.setPhysicsEngineParameter(enableFileCaching=0)\n",
    "p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "p.configureDebugVisualizer(p.COV_ENABLE_GUI,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.resetSimulation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in pybullet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load robot\n",
    "robot_urdf = rl.datapath + '/urdf/talos_reduced.urdf';\n",
    "robot_id = p.loadURDF(fileName=robot_urdf)\n",
    "dof = p.getNumJoints(robot_id)\n",
    "\n",
    "#load plane\n",
    "plane_id = p.loadURDF('plane.urdf')\n",
    "p.resetBasePositionAndOrientation(plane_id, (0,0,0), np.array([0,0,0,1]))\n",
    "\n",
    "#set default visualization function\n",
    "set_q_std = partial(set_q,robot_id, pb_joint_indices, set_base=True)\n",
    "set_q_std(q0Complete)\n",
    "vis_traj_std = partial(vis_traj, vis_func = set_q_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_, ball_id = create_primitives(radius = 0.1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "save_screenshot(200,200,800, 1000, 'data/task5.png', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 3 #choose 1, 2 or 3\n",
    "brown_color = (165./255, 42/255., 42/255.,1.)\n",
    "brown2_color = (165./255, 65/255., 65/255.,1.)\n",
    "purple_color = (165./255, 100/255., 255/255.,.9)\n",
    "ori = (0,0,0,1)\n",
    "\n",
    "if task == 1:\n",
    "    #task 1\n",
    "    _,_, box_id = create_primitives(shapeType=p.GEOM_BOX, halfExtents=[0.25, 0.4, 0.03], pos = [0.7,0,.95], rgbaColor=brown_color)\n",
    "    col_ids = [plane_id, box_id]\n",
    "elif task == 2:\n",
    "    #task 2\n",
    "    _,_, box_id = create_primitives(shapeType=p.GEOM_BOX, halfExtents=[0.25, 0.4, 0.03], pos= [0.55,0,.8], rgbaColor=brown_color)\n",
    "    _,_, box_id2 = create_primitives(shapeType=p.GEOM_BOX, halfExtents=[0.03, 0.4, 0.12], pos = [0.33,0,.95],  rgbaColor=brown2_color)\n",
    "    col_ids = [plane_id, box_id, box_id2]\n",
    "elif task == 3:\n",
    "        #task3\n",
    "    _,_, box_id =  create_primitives(shapeType=p.GEOM_BOX, halfExtents=[0.25, 0.4, 0.03], pos = [0.7,0,.75], rgbaColor=brown_color)\n",
    "    _,_, box_id2 = create_primitives(shapeType=p.GEOM_BOX, halfExtents=[0.03, 0.4, 0.03], pos = [0.5,0,1.22], rgbaColor=purple_color)\n",
    "    _,_, box_id3 = create_primitives(shapeType=p.GEOM_BOX, halfExtents=[0.03, 0.03, 0.25], pos = [0.5,0.3,1.], rgbaColor=purple_color)\n",
    "    _,_, box_id4 = create_primitives(shapeType=p.GEOM_BOX, halfExtents=[0.03, 0.03, 0.25], pos = [0.5,-0.3,1.], rgbaColor=purple_color)\n",
    "    _,_, box_id5 = create_primitives(shapeType=p.GEOM_BOX, halfExtents=[0.03, 0.03, 0.35], pos = [0.5,0.3,0.4], rgbaColor=brown_color)\n",
    "    _,_, box_id6 = create_primitives(shapeType=p.GEOM_BOX, halfExtents=[0.03, 0.03, 0.35], pos = [0.5,-0.3,.4], rgbaColor=brown_color)\n",
    "    _,_, box_id7 = create_primitives(shapeType=p.GEOM_BOX, halfExtents=[0.03, 0.03, 0.35], pos = [0.9,0.3,0.4], rgbaColor=brown_color)\n",
    "    _,_, box_id8 = create_primitives(shapeType=p.GEOM_BOX, halfExtents=[0.03, 0.03, 0.35], pos = [0.9,-0.3,.4], rgbaColor=brown_color)\n",
    "    col_ids = [plane_id, box_id, box_id2, box_id3, box_id4, box_id5, box_id6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define frame indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pb_frame_names = [p.getJointInfo(robot_id,i)[1] for i in range(dof)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pb_lh_frame_id = pb_frame_names.index(b'gripper_left_joint')\n",
    "pb_rh_frame_id = pb_frame_names.index(b'gripper_right_joint')\n",
    "pb_lf_frame_id = pb_frame_names.index(b'leg_left_sole_fix_joint')\n",
    "pb_rf_frame_id = pb_frame_names.index(b'leg_right_sole_fix_joint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load from pinocchio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robot_urdf = rl.datapath + '/urdf/talos_reduced.urdf';\n",
    "rmodel = pin.buildModelFromUrdf(robot_urdf , pin.JointModelFreeFlyer())\n",
    "rdata = rmodel.createData()\n",
    "\n",
    "robot_joint_limits = np.vstack([rmodel.lowerPositionLimit, rmodel.upperPositionLimit])\n",
    "\n",
    "pin_frame_names = [f.name for f in rmodel.frames]\n",
    "lh_frame_id = rmodel.getFrameId('gripper_left_joint')\n",
    "rh_frame_id = rmodel.getFrameId('gripper_right_joint')\n",
    "lf_frame_id = rmodel.getFrameId('leg_left_sole_fix_joint')\n",
    "rf_frame_id = rmodel.getFrameId('leg_right_sole_fix_joint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define target poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#left foot\n",
    "pos_lf, ori_lf = computePose(rmodel,rdata,lf_frame_id,q0Complete)\n",
    "rpy_lf = mat2euler(ori_lf)\n",
    "pose_lf_ref = np.concatenate([pos_lf, rpy_lf])\n",
    "#right foot\n",
    "pos_rf, ori_rf = computePose(rmodel,rdata,rf_frame_id,q0Complete)\n",
    "rpy_rf = mat2euler(ori_rf)\n",
    "pose_rf_ref = np.concatenate([pos_rf, rpy_rf])\n",
    "#left hand\n",
    "pos_lh, ori_lh = computePose(rmodel,rdata,lh_frame_id,q0Complete)\n",
    "pose_lh_ref = np.concatenate([np.array([0.3,0.2, 0.4]), np.array([0,-np.pi/2,0.])])\n",
    "#right hand\n",
    "pos_rh, ori_rh = computePose(rmodel,rdata,rh_frame_id,q0Complete)\n",
    "rpy_rh = mat2euler(ori_rh)\n",
    "pose_rh_ref = np.concatenate([pos_rh, rpy_rh])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Projector & IK solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf_cost = CostFrameRPYFloatingBase(rmodel, rdata, pose_lf_ref,lf_frame_id, sel_vector=np.array([1,1,1,1,1,1]))\n",
    "rf_cost = CostFrameRPYFloatingBase(rmodel, rdata, pose_rf_ref,rf_frame_id, sel_vector=np.array([1,1,1,1,1,1]))\n",
    "bound_cost = CostBound(robot_joint_limits)\n",
    "posture_cost = CostPosture(rmodel, rdata, q0Complete)\n",
    "com_bounds = np.array([[-0.1,-0.1,0.6],[0.1, 0.1, 1.1]])\n",
    "cost_com_bounds = CostCOMBounds(rmodel, rdata, com_bounds)\n",
    "cost_sum = CostSum()\n",
    "\n",
    "cost_sum.addCost(lf_cost, 20., 'lf_pose', 1e-5)\n",
    "cost_sum.addCost(rf_cost, 20.,'rf_pose', 1e-5)\n",
    "cost_sum.addCost(bound_cost, 10., 'com_limit', 1e-4)\n",
    "cost_sum.addCost(cost_com_bounds, 10., 'joint_limit', 1e-4)\n",
    "cost_sum.addCost(posture_cost, .01, 'posture', 1e3)\n",
    "\n",
    "robot_projector = TalosCostProjector(cost_sum)\n",
    "\n",
    "\n",
    "rh_cost = CostFrameRPYFloatingBase(rmodel, rdata, pose_rh_ref,rh_frame_id, sel_vector=np.array([1,1,1,0,0,0]))\n",
    "\n",
    "cost_sum2 = CostSum()\n",
    "cost_sum2.addCost(rh_cost, 20., 'rh_pose', 1e-3)\n",
    "cost_sum2.addCost(lf_cost, 20., 'lf_pose', 1e-5)\n",
    "cost_sum2.addCost(rf_cost, 20.,'rf_pose', 1e-5)\n",
    "cost_sum2.addCost(bound_cost, 10., 'com_limit', 1e-4)\n",
    "cost_sum2.addCost(cost_com_bounds, 10., 'joint_limit', 1e-4)\n",
    "cost_sum2.addCost(posture_cost, .01, 'posture', 1e3)\n",
    "\n",
    "robot_ik_solver = TalosCostProjector(cost_sum2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using GAN sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANSampler():\n",
    "    def __init__(self, target_sampler):\n",
    "        self.target_sampler = target_sampler\n",
    "    \n",
    "    def sample(self,N = 1, _poses = None, var = 1., idx_input = None):\n",
    "        if _poses is None:\n",
    "            _poses = self.target_sampler.sample(N)\n",
    "        self.poses = _poses\n",
    "        _foot_poses = np.tile(np.concatenate([pose_rf_ref[:3], pose_lf_ref[:3]]), (N,1))\n",
    "        _targets = np.hstack([_poses, _foot_poses])\n",
    "        feed_dict={batch_size: N*N_net, batch_target: _targets, eps_var:var}\n",
    "        feed_dict[batch_foot_ori] = _batch_foot_ori\n",
    "\n",
    "        _samples_q = sess.run([samples_q],feed_dict)\n",
    "        _samples_q = _samples_q[0]\n",
    "        qnew = []\n",
    "        for i in range(N):\n",
    "            #samples from N_net networks\n",
    "            if idx_input is None:\n",
    "                idx = np.random.randint(N_net)\n",
    "            else:\n",
    "                idx = idx_input\n",
    "            q = _samples_q[idx*N + i]\n",
    "            qnew += [get_pb_config(q)]\n",
    "        return np.array(qnew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define standard sampler\n",
    "base_sampler = sampler(com_bounds.copy())\n",
    "joint_sampler = sampler(robot_joint_limits[:,7:])\n",
    "rob_sampler = talos_sampler(base_sampler, np.array([0,0,0,1]), joint_sampler)\n",
    "\n",
    "#define GAN sampler\n",
    "target_bounds = np.array([[-0.6, -.6, 0.3], [0.8, .6, 1.4]])\n",
    "#target_bounds = np.array([[0.2, -.6, 0.3], [0.8, .6, 1.4]])\n",
    "target_sampler = sampler( target_bounds)\n",
    "gan_sampler = GANSampler(target_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try GAN sampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#q = gan_sampler.sample(_poses = np.array([[-0.2, -0.3, 1.]]))[0]\n",
    "q = gan_sampler.sample()[0]\n",
    "set_q_std(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Follow a given EE trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = []\n",
    "for theta in np.linspace(0, np.pi*2, 36):\n",
    "    p1 = 0.3*np.array([0, np.cos(theta), np.sin(theta)]) + np.array([0.5, -0.4, 1.])\n",
    "    ps += [p1]\n",
    "for theta in np.linspace(0, np.pi*2, 36):\n",
    "    p1 = 0.3*np.array([0, 2-np.cos(theta), np.sin(theta)]) + np.array([0.5, -0.4, 1.])\n",
    "    ps += [p1]\n",
    "ps = np.array(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Far targets\n",
    "ps = []\n",
    "radius = 0.5\n",
    "for theta in np.linspace(0, np.pi*2, 36):\n",
    "    p1 = radius*np.array([0, np.cos(theta), np.sin(theta)]) + np.array([0.5, -0.4, 1.8])\n",
    "    ps += [p1]\n",
    "for theta in np.linspace(0, np.pi*2, 36):\n",
    "    p1 = radius*np.array([0, 2-np.cos(theta), np.sin(theta)]) + np.array([0.5, -0.4, 1.8])\n",
    "    ps += [p1]\n",
    "ps = np.array(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Far targets(2) \n",
    "ps = []\n",
    "radius = 0.8\n",
    "for theta in np.linspace(0, np.pi*2, 36):\n",
    "    p1 = radius*np.array([0, .7*np.cos(theta), np.sin(2*theta)]) + np.array([0.5, -0.4, 1.6])\n",
    "    ps += [p1]\n",
    "for theta in np.linspace(0, np.pi*2, 36):\n",
    "    p1 = radius*np.array([0, .7*(2-np.cos(theta)), np.sin(2*theta)]) + np.array([0.5, -0.4, 1.6])\n",
    "    ps += [p1]\n",
    "ps = np.array(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_input = 0\n",
    "qs = gan_sampler.sample(_poses = ps, N=len(ps), var = .0001, idx_input=idx_input)\n",
    "for i,q in enumerate(qs):\n",
    "    set_q_std(q)\n",
    "    p.resetBasePositionAndOrientation(ball_id, ps[i], (0,0,0,1))\n",
    "    time.sleep(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_animation(qs, ps, 'data/video_follow_traj_far' + str(idx_input) + '.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Camera POV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.resetDebugVisualizerCamera( cameraDistance=2.5, cameraYaw=90, cameraPitch=-20, cameraTargetPosition=[0,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.resetDebugVisualizerCamera( cameraDistance=1.8, cameraYaw=0, cameraPitch=-20, cameraTargetPosition=[0,0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "target_samples = target_sampler.sample(N = N)\n",
    "samples = gan_sampler.sample(_poses = target_samples, N=len(target_samples), var=0.1)\n",
    "idx = 0\n",
    "for i in range(N):\n",
    "    p.resetBasePositionAndOrientation(ball_id, target_samples[i], (0,0,0,1))\n",
    "    for j in range(2):\n",
    "        save_screenshot(200, 200, 800, 1000, 'data/temp'+str(idx)+'.png', to_show=False)\n",
    "        idx +=1\n",
    "    set_q_std(samples[i])\n",
    "    for j in range(4):\n",
    "        save_screenshot(200, 200, 800, 1000, 'data/temp'+str(idx)+'.png', to_show=False)\n",
    "        idx +=1\n",
    "\n",
    "os.system('ffmpeg -r 4 -start_number 0 -i data/temp%d.png -c:v libx264 -r 30 -pix_fmt yuv420p ' + 'data/video_random_talos.mp4')\n",
    "os.system('rm data/temp*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_animation(qs,targets, filename='data/videos.mp4'):\n",
    "    for i,q in enumerate(qs):\n",
    "        p.resetBasePositionAndOrientation(ball_id, targets[i], (0,0,0,1))\n",
    "        set_q_std(q)\n",
    "        save_screenshot(200, 200, 800, 1000, 'data/temp'+str(i)+'.png', to_show=False)\n",
    "    #make into videos\n",
    "    os.system('ffmpeg -r 10 -start_number 0 -i data/temp%d.png -c:v libx264 -r 30 -pix_fmt yuv420p ' + filename)\n",
    "    os.system('rm data/temp*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare the projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = [rob_sampler, gan_sampler]\n",
    "method_names = ['Random', 'GAN']\n",
    "\n",
    "samples = gan_sampler.sample(1000)\n",
    "\n",
    "N = 10\n",
    "\n",
    "data = dict()\n",
    "for m in range(len(methods)):\n",
    "    comp_times = []\n",
    "    success = []\n",
    "    fevals = []\n",
    "    for i in range(N):\n",
    "        #idx = np.random.randint(500)\n",
    "        idx = i\n",
    "        if method_names[m] == 'GAN':\n",
    "            q = samples[idx]\n",
    "        else:\n",
    "            q = rob_sampler.sample().flatten()\n",
    "        tic = time.time()\n",
    "        res = robot_projector.project(q)\n",
    "        toc = time.time()\n",
    "        comp_times += [toc-tic]\n",
    "        success += [res['stat']]\n",
    "        fevals += [res['nfev']]\n",
    "    data[method_names[m]] = [comp_times, success, fevals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in range(len(methods[:2])):\n",
    "    print('& ' + method_names[m], end=' ')\n",
    "    comp_times, success, fevals = data[method_names[m]]\n",
    "    comp_times = np.array(comp_times)\n",
    "    fevals = np.array(fevals)\n",
    "\n",
    "    print('& {0:.1f} &  {1:.1f} $\\pm$ {2:.1f} &  {3:.1f} $\\pm$ {4:.1f} &  {5:.1f} $\\pm$ {6:.1f}'.format(np.sum(success)*100./N, np.mean(comp_times)*1000, np.std(comp_times)*1000, np.mean(comp_times[success])*1000, np.std(comp_times[success])*1000,np.mean(fevals[success]), np.std(fevals[success])), end = ''),\n",
    "    print('\\\\\\\\ ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the IK solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = data_target[:2000,:3]\n",
    "samples = gan_sampler.sample(N=2000, _poses=targets, var = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "data = dict()\n",
    "for m in range(len(methods)):\n",
    "    comp_times = []\n",
    "    success = []\n",
    "    fevals = []\n",
    "    for i in range(N):\n",
    "        idx = i\n",
    "        if method_names[m] == 'GAN':\n",
    "            q = samples[idx]\n",
    "        else:\n",
    "            q = rob_sampler.sample().flatten()\n",
    "        tic = time.time()\n",
    "        robot_ik_solver.cost.costs['rh_pose'].cost.desired_pose[:3] = targets[i]\n",
    "        res = robot_ik_solver.project(q)\n",
    "        toc = time.time()\n",
    "        comp_times += [toc-tic]\n",
    "        success += [res['stat']]\n",
    "        fevals += [res['nfev']]\n",
    "        data[method_names[m]] = [comp_times, success, fevals]\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in range(len(methods[:2])):\n",
    "    print('& ' + method_names[m], end=' ')\n",
    "    comp_times, success, fevals = data[method_names[m]]\n",
    "    comp_times = np.array(comp_times)\n",
    "    fevals = np.array(fevals)\n",
    "\n",
    "    print('& {0:.1f} &  {1:.1f} $\\pm$ {2:.1f} &  {3:.1f} $\\pm$ {4:.1f} &  {5:.1f} $\\pm$ {6:.1f}'.format(np.sum(success)*100./N, np.mean(comp_times)*1000, np.std(comp_times)*1000, np.mean(comp_times[success])*1000, np.std(comp_times[success])*1000,np.mean(fevals[success]), np.std(fevals[success])), end = ''),\n",
    "    print('\\\\\\\\ ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c-RRT with Talos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rob_col_checker = col_checker(robot_id, pb_joint_indices, col_ids, omit_indices=[ 50, 57], floating_base = True) #omit the collision of the plane with the feet\n",
    "rob_interpolator = interpolator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rrt = cRRT(rmodel.nq, rob_sampler, rob_col_checker, rob_interpolator, robot_projector)\n",
    "\n",
    "ganrrt = cRRT(rmodel.nq, gan_sampler, rob_col_checker, rob_interpolator, robot_projector)\n",
    "\n",
    "hybrid_sampler = HybridSampler(rob_sampler, gan_sampler, p_random=0.2)\n",
    "hybridrrt = cRRT(rmodel.nq, hybrid_sampler, rob_col_checker, rob_interpolator, robot_projector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get initial state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if task == 1:\n",
    "    #Task 1\n",
    "    init_bounds = np.array([[0.55, -0.25, 1.1], [0.85, 0.25, 1.3]])\n",
    "    goal_bounds = np.array([[0.55, -0.25, .4], [0.85, 0.25, .8]])\n",
    "elif task == 2:\n",
    "    init_bounds = np.array([[0.3, -0.25, 1.], [0.7, 0.25, 1.3]])\n",
    "    goal_bounds = np.array([[0.3, -0.25, .4], [0.7, 0.25, .8]])\n",
    "elif task == 3:\n",
    "    init_bounds = np.array([[0.55, -0.25, .8], [0.7, 0.25, 1.1]])\n",
    "    goal_bounds = np.array([[0.55, -0.25, .3], [0.7, 0.25, .6]])\n",
    "init_sampler = sampler(init_bounds)\n",
    "goal_sampler = sampler(goal_bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try sampling the tasks"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "_, _, ball_id = create_primitives(radius = 0.05)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "goal_pos = init_sampler.sample()[0]\n",
    "p.resetBasePositionAndOrientation(ball_id, goal_pos, (0,0,0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample initial and goal configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_config(pos_sampler, goal_pose = None, max_try = 50):\n",
    "    is_collide = True\n",
    "    status = False\n",
    "    i = 0\n",
    "    while is_collide or status is False and i < max_try:\n",
    "        i+=1\n",
    "        if goal_pose is None: \n",
    "            goal_pos = pos_sampler.sample()\n",
    "        else:\n",
    "            goal_pos = goal_pose.copy()\n",
    "        #goal_pos = np.array([0.6, -0.1, .9])[None,:]\n",
    "        #sample = gan_sampler.sample(_poses=goal_pos, var=1.)[0]\n",
    "        sample = rob_sampler.sample()[0]\n",
    "        set_q_std(sample)\n",
    "        rh_cost.desired_pose[:3] = goal_pos[0]\n",
    "        res = robot_ik_solver.project(sample.flatten())\n",
    "        start_state = res['q']\n",
    "        status = res['stat']\n",
    "        is_collide = rob_col_checker.check_collision(start_state.flatten())\n",
    "            \n",
    "            \n",
    "    set_q_std(start_state)\n",
    "    clear_output()\n",
    "    return start_state, goal_pos\n",
    "\n",
    "def get_multiple_configs(pos_sampler, N = 1, R = 1):\n",
    "    samples_set = []\n",
    "    for i in range(N):\n",
    "        print(i)\n",
    "        sample, goal_pos = get_valid_config(pos_sampler)\n",
    "        samples = [sample]\n",
    "        for j in range(R-1):\n",
    "            sample, goal_pos = get_valid_config(pos_sampler, goal_pos)\n",
    "            samples += [sample]\n",
    "        samples_set += [samples]\n",
    "    clear_output()\n",
    "    return samples_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_generate_data = True\n",
    "N = 5 #number of initial configurations\n",
    "K = 5 #number of goal configurations per task\n",
    "\n",
    "if is_generate_data:\n",
    "    init_configs = get_multiple_configs(init_sampler, N = N)\n",
    "    goal_configs = get_multiple_configs(goal_sampler, N = N, R = K)\n",
    "\n",
    "    data_configs = dict()\n",
    "    data_configs['init'] = init_configs\n",
    "    data_configs['goal'] = goal_configs\n",
    "    np.save('data/task1' + str(task) + '_configs.npy', data_configs)\n",
    "else:\n",
    "    data_configs = np.load('data/task1' + str(task) + '_configs.npy', allow_pickle=True).tolist()\n",
    "    init_configs = data_configs['init']\n",
    "    goal_configs = data_configs['goal']\n",
    "\n",
    "'''\n",
    "for qs in init_configs:\n",
    "    for q in qs:\n",
    "        set_q_std(q)\n",
    "        input()\n",
    "clear_output()\n",
    "\n",
    "for qs in goal_configs:\n",
    "    for q in qs:\n",
    "        set_q_std(q)\n",
    "        input()\n",
    "clear_output()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for qs in goal_configs:\n",
    "    for q in qs[0:2]:\n",
    "        set_q_std(q)\n",
    "        input()\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planning comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gan_result = dict()\n",
    "std_result = dict()\n",
    "hybrid_result = dict()\n",
    "results = [gan_result, std_result, hybrid_result]\n",
    "for result in results:\n",
    "    result['comp_times'] = []\n",
    "    result['nfevs'] = []\n",
    "    result['retry'] = []\n",
    "    result['nexts'] = []\n",
    "    result['success'] = []\n",
    "    result['path'] = []\n",
    "\n",
    "i=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for qs in init_configs[:]:\n",
    "    start_state = qs[0]\n",
    "    for goal_states in goal_configs[:]:\n",
    "        i+=1\n",
    "        print(i, np.mean(std_result['comp_times']), np.mean(gan_result['comp_times']), np.mean(hybrid_result['comp_times']))\n",
    "        print(i, np.sum(std_result['success']), np.sum(gan_result['success']), np.sum(hybrid_result['success']))\n",
    "        \n",
    "        \n",
    "        #plan using crrt\n",
    "        path, nfevs, nexts, success, retry, t  = rrt.plan(start_state,goal_states, max_extension_steps=500)\n",
    "        clear_output()\n",
    "        std_result['comp_times'] += [t]\n",
    "        std_result['nfevs'] += [nfevs]\n",
    "        std_result['retry'] += [retry]\n",
    "        std_result['nexts'] += [nexts]\n",
    "        std_result['success'] += [success]\n",
    "        std_result['path'] += [path]\n",
    "        \n",
    "        \n",
    "        #plan using ganrrt\n",
    "        path, nfevs, nexts, success, retry, t  = ganrrt.plan(start_state,goal_states, max_extension_steps=500)\n",
    "        clear_output()\n",
    "        gan_result['comp_times'] += [t]\n",
    "        gan_result['nfevs'] += [nfevs]\n",
    "        gan_result['retry'] += [retry]\n",
    "        gan_result['nexts'] += [nexts]\n",
    "        gan_result['success'] += [success]\n",
    "        gan_result['path'] += [path]\n",
    "        \n",
    "        #plan using hybridrrt\n",
    "        path, nfevs, nexts, success, retry, t  = hybridrrt.plan(start_state,goal_states, max_extension_steps=500)\n",
    "        clear_output()\n",
    "        hybrid_result['comp_times'] += [t]\n",
    "        hybrid_result['nfevs'] += [nfevs]\n",
    "        hybrid_result['retry'] += [retry]\n",
    "        hybrid_result['nexts'] += [nexts]\n",
    "        hybrid_result['success'] += [success]\n",
    "        hybrid_result['path'] += [path]\n",
    "        \n",
    "        if i % 20 == 0:\n",
    "            data = dict()\n",
    "            data['random'] = std_result\n",
    "            data['gan'] = gan_result\n",
    "            data['hybrid'] = hybrid_result\n",
    "            np.save('data/task3_temp.npy', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('& Random & {0:.1f} & {1:.2f} $\\pm$ {2:.2f} & {3:.1f} $\\pm$ {4:.1f} & {5:.1f} $\\pm$ {6:.1f} \\\\\\\\'.format(np.sum(std_result['success'])*100./len(std_result['success']), np.mean(std_result['comp_times']), np.std(std_result['comp_times']), np.mean(std_result['nfevs']), np.std(std_result['nfevs']), np.mean(std_result['nexts']), np.std(std_result['nexts'])))\n",
    "\n",
    "print('& GAN & {0:.1f} & {1:.2f} $\\pm$ {2:.2f} & {3:.1f} $\\pm$ {4:.1f} & {5:.1f} $\\pm$ {6:.1f} \\\\\\\\'.format(np.sum(gan_result['success'])*100./len(gan_result['success']), np.mean(gan_result['comp_times']), np.std(gan_result['comp_times']), np.mean(gan_result['nfevs']), np.std(gan_result['nfevs']), np.mean(gan_result['nexts']), np.std(gan_result['nexts'])))\n",
    "\n",
    "print('& Hybrid & {0:.1f} & {1:.2f} $\\pm$ {2:.2f} & {3:.1f} $\\pm$ {4:.1f} & {5:.1f} $\\pm$ {6:.1f} \\\\\\\\'.format(np.sum(hybrid_result['success'])*100./len(hybrid_result['success']), np.mean(hybrid_result['comp_times']), np.std(hybrid_result['comp_times']), np.mean(hybrid_result['nfevs']), np.std(hybrid_result['nfevs']), np.mean(hybrid_result['nexts']), np.std(hybrid_result['nexts'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('& Random & {0:.1f} & {1:.2f} $\\pm$ {2:.2f} & {3:.1f} $\\pm$ {4:.1f} & {5:.1f} $\\pm$ {6:.1f} \\\\\\\\'.format(np.sum(std_result['success'])*100./len(std_result['success']), np.mean(std_result['comp_times']), np.std(std_result['comp_times']), np.mean(std_result['nfevs']), np.std(std_result['nfevs']), np.mean(std_result['nexts']), np.std(std_result['nexts'])))\n",
    "\n",
    "print('& GAN & {0:.1f} & {1:.2f} $\\pm$ {2:.2f} & {3:.1f} $\\pm$ {4:.1f} & {5:.1f} $\\pm$ {6:.1f} \\\\\\\\'.format(np.sum(gan_result['success'])*100./len(gan_result['success']), np.mean(gan_result['comp_times']), np.std(gan_result['comp_times']), np.mean(gan_result['nfevs']), np.std(gan_result['nfevs']), np.mean(gan_result['nexts']), np.std(gan_result['nexts'])))\n",
    "\n",
    "print('& Hybrid & {0:.1f} & {1:.2f} $\\pm$ {2:.2f} & {3:.1f} $\\pm$ {4:.1f} & {5:.1f} $\\pm$ {6:.1f} \\\\\\\\'.format(np.sum(hybrid_result['success'])*100./len(hybrid_result['success']), np.mean(hybrid_result['comp_times']), np.std(hybrid_result['comp_times']), np.mean(hybrid_result['nfevs']), np.std(hybrid_result['nfevs']), np.mean(hybrid_result['nexts']), np.std(hybrid_result['nexts'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dict()\n",
    "data['random'] = std_result\n",
    "data['gan'] = gan_result\n",
    "data['hybrid'] = hybrid_result\n",
    "np.save('data/task' + str(task) + '3.npy', data)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
